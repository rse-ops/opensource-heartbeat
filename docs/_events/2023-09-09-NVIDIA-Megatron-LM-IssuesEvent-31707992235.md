---
event_type: IssuesEvent
avatar: "https://avatars.githubusercontent.com/u/1105742?"
user: adammoody
date: 2023-09-09
repo_name: NVIDIA/Megatron-LM
html_url: https://github.com/NVIDIA/Megatron-LM/issues/492
repo_url: https://github.com/NVIDIA/Megatron-LM
---

<a href='https://github.com/adammoody' target='_blank'>adammoody</a> open issue <a href='https://github.com/NVIDIA/Megatron-LM/issues/492' target='_blank'>NVIDIA/Megatron-LM#492</a>.

<p>[ENHANCEMENT] Tools for distributed data preprocessing</p><small>For the BigScience effort, I developed ``preprocess_data_dist.py`` to parallelize data preprocessing of large datasets.  As an example, by using 32 compute nodes, this reduces the time it takes to preprocess the 1TiB OSCAR dataset from days to an hour.  It produces ``indexed_dataset`` files that are identical to those created by the ``preprocess_data.py`` script.  An overview of the usage is written up in the "distributed data preprocessing" subsection here:...</small><a href='https://github.com/NVIDIA/Megatron-LM/issues/492' target='_blank'>View Comment</a>